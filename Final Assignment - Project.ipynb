{"nbformat_minor": 1, "cells": [{"source": "# <center> Predicting the renewal Prediction of the member </center> \n    ", "cell_type": "markdown", "metadata": {}}, {"source": "## <Center> By Jayanthi Elumalai <br>30-June-2019 </Center>\n", "cell_type": "markdown", "metadata": {}}, {"source": "## 1.Introduction", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.1 Background", "cell_type": "markdown", "metadata": {}}, {"source": "    As I have worked in more healthcare industry for 10 years , I have choosen this problem, where the Heath Insurance company always want to know the member renewal rate and how much to charge member according to the rules provided by the goverment. Members will renew the policy every year (Covering only the Individula policy whoes having the default pricing for family member till 5 and if the members of the family is more than 5 then Premium will get  increased accordingly.  ", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.2 Problem ", "cell_type": "markdown", "metadata": {}}, {"source": "    To identify whether a member will renew the policy for next year or not using the hisorical of his transaction and with similarities in other members", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.3 Interest", "cell_type": "markdown", "metadata": {}}, {"source": "    Of course the Health Insurance company always interested to know about the members count who will be renewed for next year and with analysis they can give some input to underwritters who can define the plan according to the member interest", "cell_type": "markdown", "metadata": {}}, {"source": "# 2. Data acquisition and cleaning\n", "cell_type": "markdown", "metadata": {}}, {"source": "## 2.1 Data sources", "cell_type": "markdown", "metadata": {}}, {"source": "    I have used the one of the data sources from Kaggle for Healthcare insurance (insurance.csv)and added few more features to it according to the need of the problem statement \n    Source <https://www.kaggle.com/annetxu/health-insurance-cost-prediction>", "cell_type": "markdown", "metadata": {}}, {"source": "### 2.2 Data Cleaning", "cell_type": "markdown", "metadata": {}}, {"source": "    Data is take from one CSV file and I have added some values randomly to it intialy to the data set. Hence if some thing is irrelevant during the coding we will redo the cleaning process accordingly , as the initial phase there is no need of cleaning process.\n       \n   **Items which is cleaned as part of data cleaning**\n       \n       1)  Policy detail and meber details joined together and retrieved the data\n       2) Corporate\\Small group policy where grouped together and removed from the data set, since this prediction if only for the individual policy.            \n       3) Member ID , DOB , SSN and other Secured information should be removed before tranfering the data from the DB.\n       4) Member Name and dependent name Policy Number is not needed for creating a Machine Learning model hence removed those.\n       5) Plan, Region and county and some other fields where categorical so need to change those using Label encoding and One Hot encoder\n    \n    After fixing these problems, I checked for outliers in the data. I found there were some extreme outliers, mostly caused by some types of small sample size problem. Example : Member is very new and he is not having any renewal before.", "cell_type": "markdown", "metadata": {}}, {"source": "### 2.3 Feature selection", "cell_type": "markdown", "metadata": {}}, {"source": " There were 1339 and 9 features in the data set ,\n \n **Features which is selected for Modeling**\n Age, Member count of each policy, plan details, State, county, Premium, Payperiod, Original Joined date and end Date.\n \n \n **Feature which is dropped**\n \n       1) Policy detail and meber details joined together and retrieved the data\n       2) Corporate\\Small group policy where grouped together and removed from the data set , since this prediction if only for the individual policy.\n       3) Member ID , DOB , SSN and other Secured information should be removed before tranfering the data from the DB.\n       4) Member Name and dependent name Policy Number is not needed for creating a Machine Learning model hence removed those.      \n    \n ", "cell_type": "markdown", "metadata": {}}, {"source": "## 3. Exploratory Data Analysis\n", "cell_type": "markdown", "metadata": {}}, {"source": "\nAs part of EDA, I have used LabelEncoder and OneHotEncoder \n\n**LabelEncoder :** To convert the repeated Categorical\\ text to Number for the fields like sex (Male, Female) , States, Plan (Gold, Platinum, Silver, Bronze)\n**OneHotEncoder :** To convert the single common numerical data to multiple columns with 1's and 0's , Dataset Shape - column count will get increased. \n\nWe have to use the above EDA process to make the model process fast with numberical data..\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "## Target Variable calculation ", "cell_type": "markdown", "metadata": {}}, {"source": "In Dataset which I created most of the fields will have the impact on the target variable\n<P>\n\n<li> 1. Age will have the direct impact on target variable</li>\n<li> 2. Sex will have the direct impact on target variable</li>\n<li> 3. Smoker will have the direct impact on target variable</li>\n<li> 4. Start date will have the direct impact on target variable</li>\n<li> 5. End date will have the direct impact on target variable</li>\n<li> 6. Family person count will have the direct impact on target variable</li>\n</p>", "cell_type": "markdown", "metadata": {}}, {"source": "# 4. Predictive Modeling ", "cell_type": "markdown", "metadata": {}}, {"source": "    In Dataset, there are more categorical variable which is impacting the target variable, Hence I have choosen the Multiple Linear Regression (MLR) model with Backward and Forward selection process to get the best categorical fields which will provide the accurate result.\n    \n    Initally I have set the SL value as 0.05 and the running the model each time\n    By checking the p_value remove the highest p_value categorical field and check score value which is getting improved.. So followed the same steps to get the best score using the backward and forward selection processing.\n", "cell_type": "markdown", "metadata": {}}, {"source": "# 5. Result & Conculsion", "cell_type": "markdown", "metadata": {}}, {"source": "    Using the Linear Regression with multiple Variable's we were able to get the prediction with ~95% accuracy after 7th iteration, Since there is a posibility of the improvement by avoiding the seasonal purchase of policies, Hence concluding that we can enhance this in future for better prediction.", "cell_type": "markdown", "metadata": {}}, {"source": "# 6 Lesson learnt during project time", "cell_type": "markdown", "metadata": {}}, {"source": "**Compatability issue's:**\n\n    I was using only the IBM Watson studio Notebook for doing the excersise due to which i was facing some compatable.. Notebook has the default python kernels and its packages. I learnt how to use the Environment with different version of python which will solve the compatability between the packages.\n    \n**Asset Handling:**\n\n    I was using the exisitng dataset from the IBM cloud, But when it comes to project I created my own file and loaded to Data asset, But i find very difficult to include the file to the code.\n    \n**Dataset Creation:**\n    \n    Initialy I was using the Insurance dataset from kaggle site, later found I need some more fields need for my dataset which Insurance start date and end date, etc.,\n    For Which I used excel formulae to generate the date's using Choose() and Randbetween() methods\n\n**Different Algorithms:**\n\n    To choose the best algorithm for the project excersise, I have browsed more and came to know we can apply ARIMA model and some Deep learning model also for this problem\n\n    ", "cell_type": "markdown", "metadata": {}}, {"source": "# 7. Future Enhancements", "cell_type": "markdown", "metadata": {}}, {"source": "    In Dataset, I have the date field for Policy join date and Policy end date, Using which we can identify the different trends with ARIMA (Auto-regressive integrated moving average) like Christmas season Sale and other specific trend with historical data\n\n    Using the Notebook file or PKL file integrated with the REST API and provide model prediction output to the end user applications ", "cell_type": "markdown", "metadata": {}}, {"source": "#  Coding - Using Multi Linear regression ", "cell_type": "markdown", "metadata": {}}, {"source": "import numpy as np\nimport pandas as pd\nimport matplotlib as plt", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#!conda install -c conda-forge statsmodels --yes", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "import statsmodels as sta", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_5153604d304c4ec694b812dc01f08303 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='YTwvEdUfn0uQ2yKAEoGFkReGrp4xSdRS0qwn9lVScyzk',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_5153604d304c4ec694b812dc01f08303.get_object(Bucket='opensourcetoolsassignment-donotdelete-pr-hxjttvws0qmphw',Key='Insurance_DataSet.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndataset = pd.read_csv(body)\ndataset.head()\n\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset.describe()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset.head()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset.tail()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset.shape", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset= dataset.drop(['Original_start_date', 'Policy_end_Date'], axis =1)\ndataset.shape", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset.isnull().sum()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "x=dataset.iloc[:,:-1].values # assign all the fields except the last which is label or predictor\ny=dataset.iloc[:, 12].values # all rows with only last column", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#import sys\n#!{sys.executable} -m pip install sklearn", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "y", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\n# Create some toy data in a Pandas dataframe\n#fruit_data = pd.DataFrame({\n#    'fruit':  ['apple','orange','pear','orange'],\n#    'color':  ['red','orange','green','green'],\n#    'weight': [5,6,3,4]\n#})\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset1 = MultiColumnLabelEncoder (columns = ['sex','region','smoker','State','Plan']).fit_transform(dataset)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dataset1.shape", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "x=dataset1.iloc[:,:-1].values # assign all the fields except the last which is label or predictor\ny=dataset1.iloc[:, 12].values # all rows with only last column\n\n\nfrom sklearn.preprocessing import  OneHotEncoder\n\n# Get all the unique values if we don't have them\nunique_values = pd.unique(dataset1.values.ravel()) \nunique_values\nohe = OneHotEncoder(categorical_features=[unique_values]*len(x), sparse=False)\nx = ohe_x.fit_transform(x).toarray()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Dummy Variable Trap\nX= x[:,2:] # need to change the dummy variable trap for which field is changed", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#split dataset into training and test \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.30, random_state = 0)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#Building a Model\nfrom sklearn.linear_model import LinearRegression\nregress = LinearRegression()\nregress.fit(x_train, y_train)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Prediction using the x_test data to check the model build\ny_pred = regress.predict(x_test)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Checking the accuracy using the score function for train data and test data\nregress.score (x_train, y_train)\nregress.score (x_train, y_train)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Stats Model library to generate Linear regression with more statistical data\n#Trying to data the constant value 1 to all the rows (X= C+ X)\nimport statsmodels as sta\nimport statsmodels.formula.api as sm\nX= np.append(arr=np.ones(shape = (1333, 1), values = x, dtype = int))\n\n# iteration 1\nX_ov = x[:,[0,1,2,3,4,5,6,7,8,9,10,11]]\n\nregress_ols = sta.OLS(endog=Y,exog =X_ov).fit()\nregress_ols.summary()\n\nX_ovc = x_ov[:,1:]\nx_ov_train, y_ov_train, x_ov_test, y_ov_test = train_test_split(X_ovc, Y, test_size = 0.30, randomstat = 0)\n                                                               \nregress_ov1= LinearRegression()\nregress_ov1.fit(x_ov_train, y_ov_train)\nregress_ov1.score(x_ov_train, y_ov_train)\nregress_ov1.score( x_ov_test, y_ov_test)\n\n\n\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#After checking the values P_values from Summary we have to remove the fields which is greater 0.05 (SL)\n\n# iteration 2\nX_ov = x[:,[0,1,2,3,4,5,7,8,9,10,11]]\n\nregress_ols = sm.OLS(endog=Y,exog =X_ov).fit()\nregress_ols.summary()\n\nX_ovc = x_ov[:,1:]\nx_ov_train, y_ov_train, x_ov_test, y_ov_test = train_test_split(X_ovc, Y, test_size = 0.30, randomstat = 0)\n                                                               \nregress_ov2= LinearRegression()\nregress_ov2.fit(x_ov_train, y_ov_train)\nregress_ov2.score(x_ov_train, y_ov_train)\nregress_ov2.score( x_ov_test, y_ov_test)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# iteration 3\nX_ov = x[:,[0,1,2,3,4,7,8,9,10,11]]\n\nregress_ols = sm.OLS(endog=Y,exog =X_ov).fit()\nregress_ols.summary()\n\nX_ovc = x_ov[:,1:]\nx_ov_train, y_ov_train, x_ov_test, y_ov_test = train_test_split(X_ovc, Y, test_size = 0.30, randomstat = 0)\n                                                               \nregress_ov3= LinearRegression()\nregress_ov3.fit(x_ov_train, y_ov_train)\nregress_ov3.score(x_ov_train, y_ov_train)\nregress_ov3.score( x_ov_test, y_ov_test)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# iteration 4\nX_ov = x[:,[0,1,2,3,4,7,8,9,10]]\n\nregress_ols = sm.OLS(endog=Y,exog =X_ov).fit()\nregress_ols.summary()\n\nX_ovc = x_ov[:,1:]\nx_ov_train, y_ov_train, x_ov_test, y_ov_test = train_test_split(X_ovc, Y, test_size = 0.30, randomstat = 0)\n                                                               \nregress_ov4= LinearRegression()\nregress_ov4.fit(x_ov_train, y_ov_train)\nregress_ov4.score(x_ov_train, y_ov_train)\nregress_ov4.score( x_ov_test, y_ov_test)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# iteration 5\nX_ov = x[:,[0,1,2,3,4,7,9,10]]\n\nregress_ols = sm.OLS(endog=Y,exog =X_ov).fit()\nregress_ols.summary()\n\nX_ovc = x_ov[:,1:]\nx_ov_train, y_ov_train, x_ov_test, y_ov_test = train_test_split(X_ovc, Y, test_size = 0.30, randomstat = 0)\n                                                               \nregress_ov5= LinearRegression()\nregress_ov5.fit(x_ov_train, y_ov_train)\nregress_ov5.score(x_ov_train, y_ov_train)\nregress_ov5.score( x_ov_test, y_ov_test)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# iteration 6\nX_ov = x[:,[0,1,2,3,4,9,10]]\n\nregress_ols = sm.OLS(endog=Y,exog =X_ov).fit()\nregress_ols.summary()\n\nX_ovc = x_ov[:,1:]\nx_ov_train, y_ov_train, x_ov_test, y_ov_test = train_test_split(X_ovc, Y, test_size = 0.30, randomstat = 0)\n                                                               \nregress_ov6= LinearRegression()\nregress_ov6.fit(x_ov_train, y_ov_train)\nregress_ov6.score(x_ov_train, y_ov_train)\nregress_ov6.score( x_ov_test, y_ov_test)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# iteration 7\nX_ov = x[:,[0,1,2,4,10]]\n\nregress_ols = sm.OLS(endog=Y,exog =X_ov).fit()\nregress_ols.summary()\n\nX_ovc = x_ov[:,1:]\nx_ov_train, y_ov_train, x_ov_test, y_ov_test = train_test_split(X_ovc, Y, test_size = 0.30, randomstat = 0)\n                                                               \nregress_ov7= LinearRegression()\nregress_ov7.fit(x_ov_train, y_ov_train)\nregress_ov7.score(x_ov_train, y_ov_train)\nregress_ov7.score( x_ov_test, y_ov_test)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}